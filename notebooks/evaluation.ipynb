{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GraphRAG MVP Evaluation\n",
        "\n",
        "This notebook evaluates two retrieval routes on the built index under `data/index`:\n",
        "\n",
        "- **vector_only**: vector retrieval only (graph expansion disabled)\n",
        "- **hybrid**: vector + graph paths + fusion (default)\n",
        "\n",
        "It records latency, evidence validity, and a small Hit@k score, then produces a few matplotlib plots.\n",
        "\n",
        "Prerequisite:\n",
        "\n",
        "```bash\n",
        "python -m ingestion.build_index --data_dir data/sources --out_dir data/index\n",
        "```\n",
        "\n",
        "Tip:\n",
        "- If you have `OPENAI_API_KEY` set and want a purely offline evaluation, unset it before running this notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "INDEX_DIR = Path(os.environ.get(\"INDEX_DIR\", \"data/index\"))\n",
        "os.environ.setdefault(\"INDEX_DIR\", str(INDEX_DIR))\n",
        "os.environ.setdefault(\"CHROMA_DIR\", str(INDEX_DIR / \"chroma\"))\n",
        "os.environ.setdefault(\"GRAPH_PATH\", str(INDEX_DIR / \"graph.json.gz\"))\n",
        "\n",
        "# Prefer offline behavior by default (no model downloads). You can override via env.\n",
        "os.environ.setdefault(\"ALLOW_MODEL_DOWNLOAD\", \"0\")\n",
        "\n",
        "required_files = [\n",
        "    INDEX_DIR / \"chunks.jsonl\",\n",
        "    INDEX_DIR / \"graph.json.gz\",\n",
        "    INDEX_DIR / \"manifest.json\",\n",
        "]\n",
        "required_dirs = [INDEX_DIR / \"chroma\"]\n",
        "\n",
        "missing = [p for p in required_files if not p.exists()] + [p for p in required_dirs if not p.exists()]\n",
        "if missing:\n",
        "    print(\"Index artifacts not found. Missing:\")\n",
        "    for p in missing:\n",
        "        print(f\"- {p.as_posix()}\")\n",
        "    print(\"\\nPlease run:\")\n",
        "    print(\"  python -m ingestion.build_index --data_dir data/sources --out_dir data/index\")\n",
        "    raise SystemExit(1)\n",
        "\n",
        "print(\"Index looks ready:\", INDEX_DIR.as_posix())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "from agents.graph import answer_query\n",
        "from app.settings import Settings\n",
        "from ingestion.graph_index import load_graph\n",
        "from ingestion.utils import read_jsonl\n",
        "\n",
        "settings = Settings.from_env()\n",
        "\n",
        "# Load chunks.jsonl into a dict for validation / hit@k.\n",
        "chunks_records = read_jsonl(settings.chunks_path)\n",
        "chunks_by_id = {r[\"chunk_id\"]: r for r in chunks_records if r.get(\"chunk_id\")}\n",
        "\n",
        "graph: nx.MultiDiGraph = load_graph(settings.graph_path)\n",
        "\n",
        "print(\"Loaded:\")\n",
        "print(\"- chunks:\", len(chunks_by_id))\n",
        "print(\"- graph nodes:\", graph.number_of_nodes())\n",
        "print(\"- graph edges:\", graph.number_of_edges())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "QUERIES = [\n",
        "    {\"id\": \"Q1\", \"query\": \"What is the prerequisite chain to DL301?\"},\n",
        "    {\"id\": \"Q2\", \"query\": \"Is AI101 a prerequisite of ML201? Explain briefly.\"},\n",
        "    {\"id\": \"Q3\", \"query\": \"AI101 -> ML201 -> DL301: confirm this chain using the handbook.\"},\n",
        "    {\"id\": \"Q4\", \"query\": \"In 2025, which courses depend on DL301?\"},\n",
        "    {\"id\": \"Q5\", \"query\": \"What projects are mentioned for DL301 (year 2024)?\"},\n",
        "    {\"id\": \"Q6\", \"query\": \"Which year is NLP310 and what is its prerequisite?\"},\n",
        "    {\"id\": \"Q7\", \"query\": \"Which courses are in the AI Track program in 2024?\"},\n",
        "    {\"id\": \"Q8\", \"query\": \"What are the prerequisites for CAP400?\"},\n",
        "    {\"id\": \"Q9\", \"query\": \"AI101 的先修课是什么？\"},\n",
        "    {\"id\": \"Q10\", \"query\": \"DL301 的先修链是什么？\"},\n",
        "    {\"id\": \"Q11\", \"query\": \"2025 年的 capstone 项目说明是什么？\"},\n",
        "    {\"id\": \"Q12\", \"query\": \"MLOps320: what is it about and what is its prerequisite?\"},\n",
        "]\n",
        "\n",
        "# Manual labels for a tiny Hit@k evaluation (at least 3 queries).\n",
        "# The keyword can be a course code (e.g., \"DL301\") or a source identifier (e.g., \"courses_mock.csv\").\n",
        "HIT_LABELS = {\n",
        "    \"Q1\": {\"keyword\": \"DL301\"},\n",
        "    \"Q6\": {\"keyword\": \"NLP310\"},\n",
        "    \"Q8\": {\"keyword\": \"CAP400\"},\n",
        "    \"Q7\": {\"keyword\": \"courses_mock.csv\"},\n",
        "}\n",
        "\n",
        "pd.DataFrame(QUERIES)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "from typing import Any, Dict, List\n",
        "\n",
        "\n",
        "def citation_valid_rate(citations: List[Dict[str, Any]]) -> float:\n",
        "    if not citations:\n",
        "        return 0.0\n",
        "    ok = 0\n",
        "    for c in citations:\n",
        "        cid = str(c.get(\"chunk_id\") or \"\")\n",
        "        if cid and cid in chunks_by_id:\n",
        "            ok += 1\n",
        "    return ok / float(len(citations))\n",
        "\n",
        "\n",
        "def is_graph_path_valid(path_obj: Dict[str, Any]) -> bool:\n",
        "    steps = path_obj.get(\"path\", []) or []\n",
        "    if not steps:\n",
        "        return False\n",
        "    for s in steps:\n",
        "        u = s.get(\"source\")\n",
        "        v = s.get(\"target\")\n",
        "        if not u or not v:\n",
        "            return False\n",
        "        if not graph.has_node(u) or not graph.has_node(v):\n",
        "            return False\n",
        "        if not graph.has_edge(u, v):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def path_valid_rate(paths: List[Dict[str, Any]]) -> float:\n",
        "    if not paths:\n",
        "        return 0.0\n",
        "    ok = sum(1 for p in paths if is_graph_path_valid(p))\n",
        "    return ok / float(len(paths))\n",
        "\n",
        "\n",
        "def run_one(query: str, *, route: str | None, brief: bool, top_k: int, k_hop: int) -> Dict[str, Any]:\n",
        "    # route: force with \"vector_only\"; set to None to use the default routing.\n",
        "    return answer_query(\n",
        "        query=query,\n",
        "        brief=brief,\n",
        "        top_k=top_k,\n",
        "        k_hop=k_hop,\n",
        "        route=route,\n",
        "        settings=settings,\n",
        "    )\n",
        "\n",
        "\n",
        "def run_eval(queries: List[Dict[str, str]], *, top_k: int, k_hop: int, brief: bool) -> pd.DataFrame:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    for q in queries:\n",
        "        qid = q[\"id\"]\n",
        "        qtext = q[\"query\"]\n",
        "        for mode in (\"vector_only\", \"hybrid\"):\n",
        "            route = \"vector_only\" if mode == \"vector_only\" else None\n",
        "            try:\n",
        "                result = run_one(qtext, route=route, brief=brief, top_k=top_k, k_hop=k_hop)\n",
        "            except Exception as exc:\n",
        "                # Keep the evaluation loop running even if one query fails.\n",
        "                result = {\n",
        "                    \"answer\": \"\",\n",
        "                    \"citations\": [],\n",
        "                    \"graph_paths\": [],\n",
        "                    \"debug\": {\n",
        "                        \"error\": str(exc),\n",
        "                        \"route\": (route or \"hybrid\"),\n",
        "                        \"latency_ms\": {\"total\": float(\"nan\")},\n",
        "                    },\n",
        "                }\n",
        "\n",
        "            citations = list(result.get(\"citations\", []) or [])\n",
        "            graph_paths = list(result.get(\"graph_paths\", []) or [])\n",
        "            debug = dict(result.get(\"debug\", {}) or {})\n",
        "\n",
        "            total_latency = float(debug.get(\"latency_ms\", {}).get(\"total\", float(\"nan\")))\n",
        "            route_used = str(debug.get(\"route\") or \"\")\n",
        "\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"id\": qid,\n",
        "                    \"query\": qtext,\n",
        "                    \"mode\": mode,\n",
        "                    \"route\": route_used,\n",
        "                    \"total_latency_ms\": total_latency,\n",
        "                    \"num_citations\": len(citations),\n",
        "                    \"num_graph_paths\": len(graph_paths),\n",
        "                    \"citation_valid_rate\": citation_valid_rate(citations),\n",
        "                    \"path_valid_rate\": path_valid_rate(graph_paths),\n",
        "                    # keep the full objects for later analysis\n",
        "                    \"citations\": citations,\n",
        "                    \"graph_paths\": graph_paths,\n",
        "                    \"debug\": debug,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "TOP_K = 5\n",
        "K_HOP = 2\n",
        "BRIEF = True\n",
        "\n",
        "df = run_eval(QUERIES, top_k=TOP_K, k_hop=K_HOP, brief=BRIEF)\n",
        "\n",
        "df[[\"id\", \"mode\", \"route\", \"total_latency_ms\", \"num_citations\", \"num_graph_paths\", \"citation_valid_rate\", \"path_valid_rate\"]].head(10)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "summary = (\n",
        "    df.groupby(\"mode\")\n",
        "    .agg(\n",
        "        mean_latency_ms=(\"total_latency_ms\", \"mean\"),\n",
        "        p95_latency_ms=(\"total_latency_ms\", lambda s: float(s.quantile(0.95))),\n",
        "        mean_num_citations=(\"num_citations\", \"mean\"),\n",
        "        mean_num_graph_paths=(\"num_graph_paths\", \"mean\"),\n",
        "        mean_citation_valid_rate=(\"citation_valid_rate\", \"mean\"),\n",
        "        mean_path_valid_rate=(\"path_valid_rate\", \"mean\"),\n",
        "    )\n",
        "    .sort_index()\n",
        ")\n",
        "summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "order = [q[\"id\"] for q in QUERIES]\n",
        "\n",
        "# 1) Latency comparison per query (vector_only vs hybrid)\n",
        "lat = df.pivot(index=\"id\", columns=\"mode\", values=\"total_latency_ms\").reindex(order)\n",
        "\n",
        "x = np.arange(len(lat.index))\n",
        "width = 0.38\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 4))\n",
        "ax.bar(x - width / 2, lat[\"vector_only\"].fillna(0.0), width, label=\"vector_only\")\n",
        "ax.bar(x + width / 2, lat[\"hybrid\"].fillna(0.0), width, label=\"hybrid\")\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(lat.index)\n",
        "ax.set_ylabel(\"total_latency_ms\")\n",
        "ax.set_title(\"Latency per query: vector_only vs hybrid\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2) Validity metrics comparison (mean)\n",
        "valid = df.groupby(\"mode\")[[\"citation_valid_rate\", \"path_valid_rate\"]].mean().reindex([\"vector_only\", \"hybrid\"])\n",
        "metrics = [\"citation_valid_rate\", \"path_valid_rate\"]\n",
        "\n",
        "x2 = np.arange(len(metrics))\n",
        "fig, ax = plt.subplots(figsize=(7, 4))\n",
        "ax.bar(x2 - width / 2, valid.loc[\"vector_only\", metrics].values, width, label=\"vector_only\")\n",
        "ax.bar(x2 + width / 2, valid.loc[\"hybrid\", metrics].values, width, label=\"hybrid\")\n",
        "ax.set_xticks(x2)\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.set_ylim(0.0, 1.05)\n",
        "ax.set_title(\"Validity metrics (mean)\")\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Route distribution (optional)\n",
        "route_counts = df[\"route\"].value_counts()\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "ax.bar(route_counts.index.astype(str), route_counts.values)\n",
        "ax.set_title(\"Route distribution\")\n",
        "ax.set_ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "\n",
        "def citation_haystack(citation: Dict[str, Any]) -> str:\n",
        "    cid = str(citation.get(\"chunk_id\") or \"\")\n",
        "    rec = chunks_by_id.get(cid, {})\n",
        "    meta = dict(rec.get(\"metadata\", {}) or {})\n",
        "    source = str(meta.get(\"source\") or \"\")\n",
        "    text = str(rec.get(\"text\") or \"\")\n",
        "    return (source + \"\\n\" + text).lower()\n",
        "\n",
        "\n",
        "def hit_at_k(row: pd.Series, keyword: str, k: int = 5) -> bool:\n",
        "    kw = keyword.lower().strip()\n",
        "    cits = list(row.get(\"citations\", []) or [])\n",
        "    for c in cits[:k]:\n",
        "        if kw and kw in citation_haystack(c):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "hit_rows: List[Dict[str, Any]] = []\n",
        "for qid, info in HIT_LABELS.items():\n",
        "    keyword = info[\"keyword\"]\n",
        "    for mode in (\"vector_only\", \"hybrid\"):\n",
        "        row = df[(df[\"id\"] == qid) & (df[\"mode\"] == mode)].iloc[0]\n",
        "        hit = hit_at_k(row, keyword=keyword, k=5)\n",
        "        hit_rows.append({\"id\": qid, \"mode\": mode, \"keyword\": keyword, \"hit\": hit})\n",
        "\n",
        "hit_df = pd.DataFrame(hit_rows)\n",
        "hit_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "hit_summary = hit_df.groupby(\"mode\")[\"hit\"].mean().to_dict()\n",
        "hit_summary\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "# Small narrative summary (auto-generated from the metrics)\n",
        "\n",
        "hybrid = summary.loc[\"hybrid\"]\n",
        "vector_only = summary.loc[\"vector_only\"]\n",
        "\n",
        "print(\"=== Summary ===\")\n",
        "print(f\"Queries evaluated: {len(QUERIES)}\")\n",
        "print(\"\")\n",
        "print(\"Latency:\")\n",
        "print(f\"- vector_only mean: {vector_only['mean_latency_ms']:.1f} ms\")\n",
        "print(f\"- hybrid      mean: {hybrid['mean_latency_ms']:.1f} ms\")\n",
        "print(\"\")\n",
        "print(\"Explainability (graph_paths):\")\n",
        "print(f\"- vector_only mean graph_paths: {vector_only['mean_num_graph_paths']:.2f}\")\n",
        "print(f\"- hybrid      mean graph_paths: {hybrid['mean_num_graph_paths']:.2f}\")\n",
        "print(\"\")\n",
        "print(\"Hit@5 (manual labels):\")\n",
        "print(f\"- vector_only: {hit_summary.get('vector_only', 0.0):.2f}\")\n",
        "print(f\"- hybrid:      {hit_summary.get('hybrid', 0.0):.2f}\")\n",
        "print(\"\")\n",
        "\n",
        "# Failure cases (1-2): missing graph paths or low hit\n",
        "hybrid_rows = df[df[\"mode\"] == \"hybrid\"].set_index(\"id\")\n",
        "fail_ids: List[str] = []\n",
        "\n",
        "# 1) No graph paths\n",
        "no_path = hybrid_rows[hybrid_rows[\"num_graph_paths\"] == 0].index.tolist()\n",
        "fail_ids.extend(no_path)\n",
        "\n",
        "# 2) Not hitting any manual keyword\n",
        "for qid in HIT_LABELS.keys():\n",
        "    r = hit_df[(hit_df[\"id\"] == qid) & (hit_df[\"mode\"] == \"hybrid\")].iloc[0]\n",
        "    if not bool(r[\"hit\"]):\n",
        "        fail_ids.append(qid)\n",
        "\n",
        "# Keep unique, stable order\n",
        "seen = set()\n",
        "fail_ids = [x for x in fail_ids if not (x in seen or seen.add(x))]\n",
        "\n",
        "if fail_ids:\n",
        "    print(\"Failure cases (up to 2):\")\n",
        "    for qid in fail_ids[:2]:\n",
        "        qtext = next(q[\"query\"] for q in QUERIES if q[\"id\"] == qid)\n",
        "        row = hybrid_rows.loc[qid]\n",
        "        print(f\"- {qid}: {qtext}\")\n",
        "        print(\n",
        "            f\"  citations={int(row['num_citations'])}, graph_paths={int(row['num_graph_paths'])}, \"\n",
        "            f\"hit@5={(hit_df[(hit_df['id']==qid) & (hit_df['mode']=='hybrid')]['hit'].iloc[0] if qid in HIT_LABELS else 'n/a')}\")\n",
        "        print(\"  Analysis: This is usually caused by missing entities in the query, sparse graph coverage, or weak retrieval.\")\n",
        "else:\n",
        "    print(\"No obvious failure cases detected in this small run.\")\n",
        "\n",
        "print(\"\\nTakeaway:\")\n",
        "print(\n",
        "    \"Hybrid retrieval typically improves explainability by producing prerequisite paths, \"\n",
        "    \"and can improve hit rates when the graph adds evidence chunks not surfaced by vector similarity alone.\"\n",
        ")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
