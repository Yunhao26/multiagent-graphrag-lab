# Example environment configuration for GraphRAG MVP
# Copy this file to `.env` and adjust values as needed.
#
# Notes:
# - Paths can be relative to the project root.
# - The default mode is offline-friendly (no OpenAI key required).

# -----------------------------
# Index artifact locations
# -----------------------------
INDEX_DIR=data/index
CHROMA_DIR=data/index/chroma
GRAPH_PATH=data/index/graph.json.gz

# -----------------------------
# Backend / UI
# -----------------------------
BACKEND_URL=http://localhost:8000

# -----------------------------
# LLM (optional)
# -----------------------------
# Use OpenAI only when OPENAI_API_KEY is set.
LLM_PROVIDER=openai
OPENAI_API_KEY=
# e.g. gpt-4o-mini, gpt-4.1-mini, ...
OPENAI_MODEL=gpt-4o-mini

# Local Ollama answer generation (optional):
# - Set LLM_PROVIDER=ollama
# - Ensure Ollama is running locally (default: http://localhost:11434)
# - Choose a local model from `ollama list`
OLLAMA_MODEL=mistral:7b
OLLAMA_TEMPERATURE=0
OLLAMA_NUM_PREDICT=256
OLLAMA_TIMEOUT_S=120

# -----------------------------
# Graph extraction (optional, via Ollama)
# -----------------------------
# Enable LLMGraphTransformer-based prerequisite extraction during ingestion.
# This is optional and disabled by default.
GRAPH_LLM_PROVIDER=none
GRAPH_LLM_MODEL=mistral:7b
OLLAMA_BASE_URL=http://localhost:11434
GRAPH_LLM_MAX_DOCS=25
GRAPH_LLM_TEMPERATURE=0
GRAPH_LLM_NUM_PREDICT=256
GRAPH_LLM_TIMEOUT_S=120

# -----------------------------
# Embeddings (HuggingFace)
# -----------------------------
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
# auto -> cuda if available, else cpu
EMBEDDING_DEVICE=auto
EMBEDDING_BATCH_SIZE=64

# Allow downloading the embedding model from HF Hub (0/1).
# If set to 0, the code attempts fully offline behavior and will fall back if the model
# is not available locally.
ALLOW_MODEL_DOWNLOAD=0

# -----------------------------
# Fully offline embedding fallback
# -----------------------------
FORCE_HASH_EMBEDDINGS=0
# Default aligns with MiniLM (384) to avoid Chroma dimension mismatches.
HASH_EMBEDDING_DIM=384

# -----------------------------
# Cross-Encoder rerank (quality-first, optional)
# -----------------------------
# Re-rank the retrieved chunks using a cross-encoder classifier (much higher precision).
# Requires model download unless already cached.
ENABLE_CE_RERANK=0
# Multi-lingual recommended for Chinese questions + French PDFs:
CE_RERANK_MODEL=BAAI/bge-reranker-v2-m3
# Rerank top-N candidate chunks (higher = better quality, slower).
CE_RERANK_TOP_N=80
# auto -> cuda if available, else cpu
CE_RERANK_DEVICE=auto
